{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di DeepChaos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xbadc0ffe/DeepChaos/blob/main/DeepChaos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Chaos"
      ],
      "metadata": {
        "id": "BhzC0zRI-uji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities and Imports\n",
        "\n",
        "from re import S\n",
        "from typing import Union, Optional,  Optional, Callable, Dict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import plotly.express as px\n",
        "\n",
        "#np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "\n",
        "# reproducibility stuff\n",
        "if True:\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def count_parameters(model: torch.nn.Module) -> int:\n",
        "  \"\"\" Counts the number of trainable parameters of a module\n",
        "  \n",
        "  :param model: model that contains the parameters to count\n",
        "  :returns: the number of parameters in the model\n",
        "  \"\"\"\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def lin_sys(x, A, b):\n",
        "    y = torch.einsum(\"ik, k -> i\", A, x)\n",
        "    y = y + b\n",
        "    return y\n",
        "\n",
        "\n",
        "class ESN(nn.Module):\n",
        "    def __init__(\n",
        "        self, Win:torch.Tensor, W:torch.Tensor, activation:str, output_size: int\n",
        "    ) -> None:\n",
        "        \n",
        "        super().__init__()\n",
        "        #self.fci = nn.Linear(input_size, in_to_hidden)\n",
        "        self.Win = Win\n",
        "        self.W = W\n",
        "        self.activation = activation\n",
        "        self.ker_size = 10\n",
        "\n",
        "        if activation == \"LeakyReLU\":\n",
        "            self.act = torch.nn.LeakyReLU()\n",
        "        elif activation == \"Tanh\":\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif  activation == \"ELU\":\n",
        "            self.act = torch.nn.ELU(alpha=1.0, inplace=False)\n",
        "        elif activation == \"ModTanh\":\n",
        "            self.mod = nn.Linear(W.shape[0], 1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"PrModTanh\":\n",
        "            self.mod = nn.Linear(W.shape[0], 1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"ConvModTanh1\" or activation == \"ConvModTanh2\":\n",
        "            self.mod2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(W.shape[0],1))\n",
        "            self.mod3 = nn.Linear(W.shape[0],1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        else:   \n",
        "            # Default activation function\n",
        "            self.act = torch.nn.Tanh()\n",
        "\n",
        "        self.fco = nn.Linear(W.shape[0], output_size)\n",
        "\n",
        "        #self.mod2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.ker_size)#max(3,W.shape[0]//20))\n",
        "        #self.mod3 = nn.Linear((W.shape[0]-self.ker_size+1),1)\n",
        "\n",
        "        #self.mod4 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(W.shape[0],1))\n",
        "        #self.mod5 = nn.Linear(W.shape[0],1)\n",
        "\n",
        "    def forward(self, \n",
        "                u: torch.Tensor, \n",
        "                h_i: torch.Tensor\n",
        "        ) -> torch.Tensor:\n",
        "\n",
        "        #x = self.fci(u)\n",
        "        xu = torch.einsum(\"ik, k -> i\", self.Win, u)\n",
        "        #x, h_o = self.resvoir(x, h_i)\n",
        "        x = xu + torch.einsum(\"ij, j -> i\", self.W, h_i)\n",
        "        #h_o = torch.tanh(x)   # x(n+1) = tanh(Win*u(n) +  W*x(n))\n",
        "        h_o = self.act(x)\n",
        "\n",
        "        if self.activation == \"ModTanh\":\n",
        "            h_o = (torch.tanh(self.mod(h_o))+1)*h_o  \n",
        "\n",
        "        elif self.activation == \"PrModTanh\":\n",
        "            pr = torch.tanh(torch.einsum(\"ij, j -> i\", self.W, h_o) +  torch.einsum(\"ik, k -> i\", self.Win, self.fco(h_o)))\n",
        "            h_o = (torch.tanh(self.mod(pr))+1)*h_o\n",
        "\n",
        "        elif self.activation == \"ConvModTanh1\":\n",
        "            h_conv = torch.einsum(\"ij, j -> i\", self.W, h_o) \n",
        "            h_conv = torch.einsum(\"i, ij, k-> jk\", h_o, self.W, h_conv)\n",
        "            h_conv = self.mod2(torch.reshape(h_conv,(1,1,W.shape[0],W.shape[0])))[0,0,0,...]\n",
        "            h_o = (torch.tanh(self.mod3(h_conv))+1)*h_o\n",
        "\n",
        "        elif self.activation == \"ConvModTanh2\":\n",
        "            h_conv= self.W*h_o\n",
        "            h_conv = self.mod2(torch.reshape(h_conv,(1,1,W.shape[0],W.shape[0])))[0,0,0,...]\n",
        "            h_o = (torch.tanh(self.mod3(h_conv))+1)*h_o\n",
        "\n",
        "        #h_conv = torch.einsum(\"ij, j -> i\", self.W, h_i) \n",
        "        #h_conv = torch.einsum(\"i, ij, k-> jk\", h_i, self.W, h_conv)\n",
        "        \n",
        "        #h_conv = self.mod2(torch.reshape(h_conv,(1,1,1000,1000)))[0,0,...]\n",
        "        #h_o = (torch.tanh(self.mod3(torch.diag(h_conv)))+1)*h_o\n",
        "\n",
        "        #h_conv = self.mod2(torch.reshape(h_conv,(1,1,1000,1000)))[0,0,...]\n",
        "        #h_conv = F.max_pool2d(h_conv, kernel_size=self.ker_size)\n",
        "        #h_o = (torch.tanh(self.mod3(h_conv))+1)*h_o\n",
        "\n",
        "        x = self.fco(h_o)\n",
        "\n",
        "        return x, h_o\n",
        "\n",
        "\n",
        "def get_model_optimizer(model: torch.nn.Module, opt_type:str) -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    Encapsulate the creation of the model's optimizer, to ensure that we use the\n",
        "    same optimizer everywhere\n",
        "\n",
        "    :param model: the model that contains the parameter to optimize\n",
        "\n",
        "    :returns: the model's optimizer\n",
        "    \"\"\"\n",
        "    if opt_type == \"Adam\":\n",
        "        return optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    elif opt_type == \"SGD\":\n",
        "        return optim.SGD(model.parameters(), lr=0.01, momentum=0.1, weight_decay=1e-5)\n",
        "    else:\n",
        "        # default\n",
        "        return optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# Fast Largest Eigenvector\n",
        "def FLE(A, iters=500, eigvec=False):\n",
        "    v = torch.ones(A.shape[0]) #.to(device)\n",
        "    for _ in range(iters):\n",
        "        m = torch.einsum(\"ij, j -> i\",A, v)\n",
        "        lamb = torch.norm(m)\n",
        "        v = m/lamb\n",
        "    if eigvec:\n",
        "        return torch.norm(torch.einsum(\"ij, j -> i\",A, v)), v\n",
        "    else:\n",
        "        return torch.norm(torch.einsum(\"ij, j -> i\",A, v))\n",
        "\n",
        "# Slow Largest Eigenvector\n",
        "def SLE(A):\n",
        "    eig = torch.linalg.eig(A)[0]\n",
        "    eig = eig.unsqueeze(1)\n",
        "    return torch.max(torch.norm(eig, dim=1))\n",
        "\n",
        "\n",
        "def set_reproducibility(seed=42):\n",
        "    # reproducibility stuff\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def sigm(x, k=1):\n",
        "    return 1/(1 + np.exp(-k*x))\n",
        "\n",
        "# lw and rw tune how much the distribution stays with mean 1 and N\n",
        "# (we are using the sigmoid(x) for x in [lw,rw]). \n",
        "# Lower lw => more epochs with mean 1, higher rw => more epochs with mean around N.\n",
        "# ek tune the slope of the sigmoid distribution (lower ek => smaller slope).\n",
        "# estd = 0.5 or 0.7 power law of the std\n",
        "# N is the maximum value to reach\n",
        "def curriculum_gen(k,K,N, lw=-7, rw=4, ek=0.85, estd=0.5):\n",
        "    ran = N+1\n",
        "    while ran > N:\n",
        "        mean = sigm((K-k)/K*lw + k/K*rw, ek)*N\n",
        "        ran = 1 + abs(np.random.normal(mean,(N*k/K)**estd))\n",
        "        ran = int(ran)\n",
        "    return ran\n",
        "\n",
        "\n",
        "# Exectue curriculum_gen \"cycles\" times\n",
        "# cycles span grow as x/(max(1, log(x)-3))\n",
        "def phased_gen(k, K, N, cycles=None, lw=-7, rw=4, ek=0.85, estd=0.5):\n",
        "    if cycles is None:\n",
        "        cycles = max(1, int(np.log(K))-3) #-2.6\n",
        "        #cycles = max(1, K//500)\n",
        "    Kc = K/cycles\n",
        "    ran = curriculum_gen(k%Kc,Kc,N, lw=-7, rw=4, ek=0.85, estd=0.5)\n",
        "    return ran\n",
        "        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "DqcX_e0MkFEl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Saving Utilities\n",
        "\n",
        "PATH = os.path.abspath(\".\\data\")\n",
        "\n",
        "def load(mod_name=\"model\", jfile=\"cfgs\", path=PATH):\n",
        "    model_dict = None\n",
        "    data = None\n",
        "    plots = None\n",
        "    try:\n",
        "        data =  load_cfgs(jfile, path)\n",
        "        model_dict = load_model_dict(mod_name,path)\n",
        "        plots = load_plot_data(path)\n",
        "    except Exception as e:\n",
        "        #print(e)\n",
        "        return False, model_dict, data, plots\n",
        "    return True, model_dict, data, plots\n",
        "\n",
        "def load_plot_data(path=PATH):\n",
        "    plots = {}\n",
        "    plots[\"loss\"] = torch.load(path + \"\\\\loss.pt\")\n",
        "    plots[\"weigths_norm\"] = torch.load(path + \"\\\\weigths_norm.pt\")\n",
        "    plots[\"ground_truth_sys\"] = torch.load(path + \"\\\\ground_truth_sys.pt\")\n",
        "    plots[\"1-forecasting\"] = torch.load(path + \"\\\\1-forecasting.pt\")\n",
        "    plots[\"n-forecasting\"] = torch.load(path + \"\\\\n-forecasting.pt\")\n",
        "    plots[\"trained-forecasting\"] = torch.load(path + \"\\\\trained-forecasting.pt\")\n",
        "    return plots\n",
        "\n",
        "def load_cfgs(name=\"cfgs\", path=PATH):\n",
        "    with open(path +f\"\\{name}.json\", \"r\") as jfile:\n",
        "        return json.load(jfile)\n",
        "\n",
        "def load_model_dict(name=\"model\",path=PATH):\n",
        "    return torch.load(path + f\"\\{name}.pth\")\n",
        "\n",
        "def save_hidden(h_0, name=\"h_0\", path=PATH):\n",
        "    torch.save(h_0, path + f\"\\{name}.pt\")\n",
        "\n",
        "def load_hidden(name=\"h_0\", path=PATH):\n",
        "    return torch.load(path + f\"\\{name}.pt\")\n",
        "\n",
        "def save_initial(x_0, name=\"x_0\", path=PATH):\n",
        "    torch.save(x_0, path + f\"\\{name}.pt\")\n",
        "\n",
        "def load_initial(name=\"x_0\", path=PATH):\n",
        "    return torch.load(path + f\"\\{name}.pt\")\n",
        "\n",
        "def save_W(W, name=\"W\", path=PATH):\n",
        "    torch.save(W, path + f\"\\{name}.pt\")\n",
        "\n",
        "def load_W(name=\"W\", path=PATH):\n",
        "    return torch.load(path + f\"\\{name}.pt\")\n",
        "\n",
        "def save_Win(Win, name=\"Win\", path=PATH):\n",
        "    torch.save(Win, path + f\"\\{name}.pt\")\n",
        "\n",
        "def load_Win(name=\"Win\", path=PATH):\n",
        "    return torch.load(path + f\"\\{name}.pt\")\n",
        "\n",
        "# TODO: Save System (and x_0 togheter)\n",
        "# TODO: Save W_in, W, h_0 togheter\n",
        "\n",
        "def save(model, data, plots: dict, model_name=\"model\", cfgs_name=\"cfgs\", path=PATH):\n",
        "\n",
        "    # Saving model weights\n",
        "    torch.save(model.state_dict(), path+f\"\\{model_name}.pth\")\n",
        "\n",
        "    # Saving plots and configs\n",
        "    jsonfile = path+f\"\\{cfgs_name}.json\"\n",
        "    \n",
        "    with open(jsonfile, \"w+\") as jfile:\n",
        "        json.dump(data, jfile, indent=4)\n",
        "\n",
        "    # saving plots\n",
        "    for name in plots:\n",
        "        torch.save(plots[name], path + f\"\\{name}.pt\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HdT0pz3qlt6T"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "def build_ESN(H=200, d=3, lambda_coeff=0.4, dym_sys=3, sigma_in=0.15, activation=\"Tanh\", output_size=\"3\", device=\"cpu\"):\n",
        "    model = ESN()\n",
        "    model.build_model(H, d, lambda_coeff, dym_sys, sigma_in, activation, output_size=output_size, device=device)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ESN(nn.Module):\n",
        "    def __init__(\n",
        "        self, Win:torch.Tensor=None, W:torch.Tensor=None, activation:str = \"Tanh\", output_size: int = 3, h_0:torch.Tensor=None\n",
        "    ) -> None:\n",
        "        \n",
        "        super().__init__()\n",
        "        self.Win = Win\n",
        "        self.W = W\n",
        "        self.activation = activation\n",
        "        self.h = h_0\n",
        "\n",
        "        if W is None:\n",
        "            return\n",
        "\n",
        "        if activation == \"LeakyReLU\":\n",
        "            self.act = torch.nn.LeakyReLU()\n",
        "        elif activation == \"Tanh\":\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif  activation == \"ELU\":\n",
        "            self.act = torch.nn.ELU(alpha=1.0, inplace=False)\n",
        "        elif activation == \"ModTanh\":\n",
        "            self.mod = nn.Linear(W.shape[0], 1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"PrModTanh\":\n",
        "            self.mod = nn.Linear(W.shape[0], 1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"ConvModTanh1\" or activation == \"ConvModTanh2\":\n",
        "            self.mod2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(W.shape[0],1))\n",
        "            self.mod3 = nn.Linear(W.shape[0],1)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        else:   \n",
        "            # Default activation function\n",
        "            self.act = torch.nn.Tanh()\n",
        "\n",
        "        self.fco = nn.Linear(W.shape[0], output_size)\n",
        "\n",
        "\n",
        "    def forward(self, \n",
        "                u: torch.Tensor, \n",
        "                h_i: torch.Tensor\n",
        "        ) -> torch.Tensor:\n",
        "\n",
        "        #x = self.fci(u)\n",
        "        xu = torch.einsum(\"ik, k -> i\", self.Win, u)\n",
        "        #x, h_o = self.resvoir(x, h_i)\n",
        "        x = xu + torch.einsum(\"ij, j -> i\", self.W, h_i)\n",
        "        #h_o = torch.tanh(x)   # x(n+1) = tanh(Win*u(n) +  W*x(n))\n",
        "        h_o = self.act(x)\n",
        "\n",
        "        if self.activation == \"ModTanh\":\n",
        "            h_o = (torch.tanh(self.mod(h_o))+1)*h_o  \n",
        "\n",
        "        elif self.activation == \"PrModTanh\":\n",
        "            pr = torch.tanh(torch.einsum(\"ij, j -> i\", self.W, h_o) +  torch.einsum(\"ik, k -> i\", self.Win, self.fco(h_o)))\n",
        "            h_o = (torch.tanh(self.mod(pr))+1)*h_o\n",
        "\n",
        "        elif self.activation == \"ConvModTanh1\":\n",
        "            h_conv = torch.einsum(\"ij, j -> i\", self.W, h_o) \n",
        "            h_conv = torch.einsum(\"i, ij, k-> jk\", h_o, self.W, h_conv)\n",
        "            h_conv = self.mod2(torch.reshape(h_conv,(1,1,self.W.shape[0],self.W.shape[0])))[0,0,0,...]\n",
        "            h_o = (torch.tanh(self.mod3(h_conv))+1)*h_o\n",
        "\n",
        "        elif self.activation == \"ConvModTanh2\":\n",
        "            h_conv= self.W*h_o\n",
        "            h_conv = self.mod2(torch.reshape(h_conv,(1,1,self.W.shape[0],self.W.shape[0])))[0,0,0,...]\n",
        "            h_o = (torch.tanh(self.mod3(h_conv))+1)*h_o\n",
        "\n",
        "\n",
        "        x = self.fco(h_o)\n",
        "\n",
        "        return x, h_o\n",
        "\n",
        "    # like forward but handle the hidden state internnally\n",
        "    def step(self, u: torch.Tensor )-> torch.Tensor:\n",
        "        x, h_i = self.forward(u, self.h)\n",
        "        self.h = h_i\n",
        "        return x, h_i\n",
        "\n",
        "    def init_reservoir(self, H=200, d=3, lambda_coeff=0.4, device=\"cpu\"): \n",
        "        self.H = H         \n",
        "        W = torch.rand([H,H])*2 - 1\n",
        "        ind = np.diag_indices(W.shape[0])\n",
        "        W[ind[0],ind[1]] = 0\n",
        "        for i in range(W.shape[0]):\n",
        "            for j in range(W.shape[1]):\n",
        "                if torch.rand(1) > d/(H-1):\n",
        "                    W[i,j] = 0\n",
        "        cnt = 0\n",
        "        for i in range(W.shape[0]):\n",
        "            for j in range(W.shape[1]):\n",
        "                if abs(W[i,j]) > 0:\n",
        "                    cnt += 1\n",
        "\n",
        "        self.connectivity = cnt/H\n",
        "\n",
        "        # Forcing largest eigenvalue norm to lambda to ensure ESP\n",
        "        eig = SLE(W) #FLE(W)\n",
        "        W = W*lambda_coeff/eig\n",
        "        W.to(device)\n",
        "        self.W = W\n",
        "\n",
        "    def init_Win(self, H=None, dym_sys=3, sigma_in=0.15, device=\"cpu\"):\n",
        "        if H is None:\n",
        "            H = self.H\n",
        "        In_acc = (torch.rand([H], device=device)*2 - 1)*sigma_in\n",
        "        h_0 = torch.rand([H], device=device)\n",
        "        Win = torch.zeros([H,dym_sys], device=device)\n",
        "        for i in range(In_acc.shape[0]):\n",
        "            Win[i,torch.randint(high=dym_sys, size=[1], device=device)] = In_acc[i]\n",
        "        self.Win = Win\n",
        "\n",
        "    def set_activation(self, activation, H=None, device=\"cpu\"): \n",
        "\n",
        "        if H is None:\n",
        "            H = self.H\n",
        "\n",
        "        self.activation = activation  \n",
        "        if activation == \"LeakyReLU\":\n",
        "            self.act = torch.nn.LeakyReLU()\n",
        "        elif activation == \"Tanh\":\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif  activation == \"ELU\":\n",
        "            self.act = torch.nn.ELU(alpha=1.0, inplace=False)\n",
        "        elif activation == \"ModTanh\":\n",
        "            self.mod = nn.Linear(H, 1, device=device)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"PrModTanh\":\n",
        "            self.mod = nn.Linear(H, 1, device=device)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        elif activation == \"ConvModTanh1\" or activation == \"ConvModTanh2\":\n",
        "            self.mod2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(H,1), device=device)\n",
        "            self.mod3 = nn.Linear(H,1, device=device)\n",
        "            self.act = torch.nn.Tanh()\n",
        "        else:   \n",
        "            # Default activation function\n",
        "            self.act = torch.nn.Tanh()\n",
        "\n",
        "    def set_fc_output(self, output_size=3, H=None, device=\"cpu\"):\n",
        "        if H is None:\n",
        "            H = self.H\n",
        "        self.fco = nn.Linear(H, output_size, device=device)\n",
        "\n",
        "    def build_model(self, H=200, d=3, lambda_coeff=0.4, dym_sys=3, sigma_in=0.15, activation=\"Tanh\", output_size=\"3\", device=\"cpu\"):\n",
        "        self.init_reservoir(H=H, d=d, lambda_coeff=lambda_coeff, device=device)\n",
        "        self.init_Win(dym_sys=dym_sys, sigma_in=sigma_in, device=device)\n",
        "        self.set_activation(activation, device=device)\n",
        "        self.set_fc_output(output_size, device=device)\n",
        "\n",
        "    def rand_h0(self, range=1,H=None, device=\"cpu\"):\n",
        "        if H is None:\n",
        "            H = self.H\n",
        "        self.h_0 = (torch.rand([H], device=device)*2-1)*range\n",
        "        return self.h_0\n",
        "        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xk9lcAe2kbwn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Systems emulation utilities\n",
        "\n",
        "\n",
        "def Lorenz(t, x, sigma=10, rho=28, beta=8/3):\n",
        "    return torch.tensor([sigma*(x[1]-x[0]), rho*x[0] -x[1] - x[0]*x[2], x[0]*x[1]-beta*x[2]], device=x.device)\n",
        "\n",
        "def Elicoidal(t, x, a=0.6, b=-0.1):\n",
        "    return torch.tensor([-a*x[1], a*x[0], b*x[2]], device=x.device)\n",
        "\n",
        "def linear_dis_sys(A:torch.Tensor, b:torch.Tensor):\n",
        "    y = lambda t,x : torch.einsum(\"ik, k -> i\", A, x) + b\n",
        "    return y\n",
        "\n",
        "def messy_dis_sys(t, x):\n",
        "    return x*torch.sin(x)**2 + 0.2*x\n",
        "\n",
        "def armonic_boom_dis(t,x, delT=50, delX=1, delS=0.4):\n",
        "    return torch.tensor([x[0]*(delS+np.sin(t/delT)), -x[0]*(delS + np.cos(t/delT))])*delX\n",
        "\n",
        "\n",
        "class Sys():\n",
        "\n",
        "    def __init__(self, x_0, f, eps, t0=0, steps=1, sys_type=\"continuous\") -> None:\n",
        "        self.x_0 = x_0          # initial state\n",
        "        self.f = f              # transition matrix\n",
        "        self.x = x_0            # state\n",
        "        self.t0 = t0            # initila time\n",
        "        self.clock = 0          # clock\n",
        "        self.eps = eps          # epsilon (time step)\n",
        "        self.steps = steps      # RK steps\n",
        "        self.sys_type=sys_type  # system type (discrete/continuous)\n",
        "\n",
        "    # System step, executes self.steps RK4\n",
        "    def step(self):\n",
        "        if self.sys_type == \"continuous\":\n",
        "            for i in range(self.steps):\n",
        "                self.RK4()\n",
        "                self.clock +=1\n",
        "        elif self.sys_type == \"discrete\":\n",
        "            for i in range(self.steps):\n",
        "                self.dis_step()\n",
        "                self.clock +=1\n",
        "        else:\n",
        "            raise Exception(\"Unknwon System Type\")\n",
        "\n",
        "    def dis_step(self):\n",
        "        t = self.clock*self.eps + self.t0\n",
        "        self.x = self.f(t, self.x)\n",
        "\n",
        "    # Runge-Kutta 4\n",
        "    def RK4(self):\n",
        "        eps = self.eps\n",
        "        x = self.x\n",
        "        f = self.f\n",
        "\n",
        "        t = self.clock*eps + self.t0\n",
        "        k1 = eps * f(t, x)\n",
        "        k2 = eps * f(t + 0.5 * eps, x + 0.5 * k1)\n",
        "        k3 = eps * f(t + 0.5 * eps, x + 0.5 * k2)\n",
        "        k4 = eps * f(t + eps, x + k3)\n",
        "        self.x = x + (1.0 / 6.0)*(k1 + 2 * k2 + 2 * k3 + k4)\n",
        "\n",
        "    # System evolves until t_end\n",
        "    def step_t(self,t_end):\n",
        "        t = self.clock*self.eps + self.t0\n",
        "        if t > t_end:\n",
        "            raise Exception(\"End time cannot be lower then self.t\")\n",
        "        n = int((t_end -t)/self.eps)\n",
        "        if self.sys_type == \"continuous\":\n",
        "            for i in range(n):\n",
        "                self.RK4()\n",
        "                self.clock +=1\n",
        "        elif self.sys_type == \"discrete\":\n",
        "            for i in range(n):\n",
        "                self.dis_step()\n",
        "                self.clock +=1\n",
        "        else:\n",
        "            raise Exception(\"Unknwon System Type\")\n",
        "\n",
        "\n",
        "    def restart(self, x0, t0=0):\n",
        "        self.x0 = x0\n",
        "        self.t0 = t0\n",
        "        self.x = x0\n",
        "        self.clock = 0\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ynP3hp5hjliu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### ESN - Hyperparameters\n",
        "\n",
        "reproducible = True\n",
        "\n",
        "if reproducible:\n",
        "    set_reproducibility(seed=42)\n",
        "    reproducible = 42\n",
        "\n",
        "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "H = 200\n",
        "d = 3              \n",
        "Nt = 400           # paper: 1000\n",
        "for_hor = 1        # This can be any integer, \"n\" for infinite horizon, \"v\" for variable, \"c\" for curriculum learning, \"p\" for phased horizon\n",
        "dym_sys = 3\n",
        "epochs = 10000\n",
        "activations = [\"LeakyReLU\", \"Tanh\", \"ELU\", \"ModTanh\", \"PrModTanh\", \"ConvModTanh1\", \"ConvModTanh2\"]\n",
        "activation = activations[6]\n",
        "\n",
        "sys_types = {\n",
        "            \"discrete\":     [\"dis_rectilinear\", \"dis_sinusoidal\", \"messy_dis\", \"armonic_boom_dis\"],  \n",
        "            \"continuous\":   [\"Lorenz\", \"Elicoidal\"]\n",
        "        }\n",
        "sys_type = list(sys_types.keys())[1]  # \"discrete\" #\"continuous\" \n",
        "sys_name = sys_types[sys_type][0]\n",
        "\n",
        "opt_types = [\"SGD\", \"Adam\"]\n",
        "opt_type = opt_types[1]\n",
        "\n",
        "early_stop = None  # 0.0005*Nt  # None to deactive early stopping\n",
        "tikhonov = 0.0001  # lambda value, 0 to deactive tikhonov\n",
        "p_tikhonov = 2\n",
        "sigma_in = 0.15\n",
        "lambda_coeff = 0.4  # <= 1 to ensure the Echo State Property\n",
        "save_training = False         # save training\n",
        "pre_training = True           # pre training   \n",
        "pre_training_horizon = 20     # pre training horizon \n",
        "alpha = 100                   # tempered Physical loss\n",
        "basin_r = 0.01                # radius of the n-sphere around x_0 form which initial states are randomly initialized during training\n",
        "washout = 100                 # Steps after which the training starts and predictability is counted (the Reservoir should not depend almost anymore on h_0)\n",
        "df_wg = 0.5                   # This define where the differential operator df is computed for the physical loss. E.g. if 1: df(t, x_n), 0: df(t, x_{n-1}), 0.5: df(t, [x_n + x_{n-1}]/2)                  \n"
      ],
      "metadata": {
        "id": "peO8fweykPn2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Models Building\n",
        "\n",
        "###########  ESN model \n",
        "\n",
        "\n",
        "model = build_ESN(H, d, lambda_coeff, dym_sys, sigma_in, activation, output_size=3, device=device)\n",
        "h_0 = model.rand_h0(device=device)\n",
        "# since we are going to use the same model for different forecasting, we will save the internal state h_i externally\n",
        "\n",
        "\n",
        "########### Ground Truth model\n",
        "\n",
        "# Discrete\n",
        "if sys_type == \"discrete\":\n",
        "\n",
        "    if sys_name == \"dis_rectilinear\":\n",
        "\n",
        "        A = torch.tensor(np.array([[1, 0, 0],[0, 1 ,0],[ 0, 0, 1]]), dtype=torch.float, device=device)\n",
        "        x_0 = torch.tensor(np.array([1,1,-1.]), dtype=torch.float, device=device)\n",
        "        b = torch.tensor(np.array([0.1,0,0]), dtype=torch.float, device=device)\n",
        "        eps = 1\n",
        "        # create a linear discrete model x(k+1) = A*x(k) + b \n",
        "        df = linear_dis_sys(A,b) \n",
        "\n",
        "    elif sys_name == \"dis_sinusoidal\":  \n",
        "\n",
        "        #A = torch.tensor(np.array([[0, 1, 0],[-1, 0 ,0],[ 0, 0, 1]]), dtype=torch.float, device=device)\n",
        "        A = torch.tensor(np.array([[1, 0, 0],[0, 0 ,1],[ 0, -1, 0]]), dtype=torch.float, device=device)\n",
        "        x_0 = torch.tensor(np.array([1,1,-1.]), dtype=torch.float, device=device)\n",
        "        b = torch.tensor(np.array([0,0,0]), dtype=torch.float, device=device)\n",
        "        eps = 1\n",
        "        df = linear_dis_sys(A,b)\n",
        "    \n",
        "    elif sys_name == \"messy_dis\":\n",
        "        dym_sys = 1\n",
        "        x_0 = torch.tensor(np.array([1]), dtype=torch.float, device=device)\n",
        "        eps = 1\n",
        "        df = messy_dis_sys\n",
        "\n",
        "    elif sys_name == \"armonic_boom_dis\":\n",
        "        dym_sys = 2\n",
        "        x_0 = torch.tensor(np.array([1,1]), dtype=torch.float, device=device)\n",
        "        eps = 1\n",
        "        df = armonic_boom_dis\n",
        "\n",
        "    else:\n",
        "        # default: discrete sinusoidal\n",
        "        A = torch.tensor(np.array([[0, 1, 0],[-1, 0 ,0],[ 0, 0, 1]]), dtype=torch.float, device=device)\n",
        "        x_0 = torch.tensor(np.array([1,1,-1.]), dtype=torch.float, device=device)\n",
        "        b = torch.tensor(np.array([0,0,0]), dtype=torch.float, device=device)\n",
        "        eps = 1\n",
        "        df = linear_dis_sys(A,b)\n",
        "\n",
        "# Continuous\n",
        "elif sys_type == \"continuous\":\n",
        "\n",
        "    if sys_name == \"Lorenz\":\n",
        "        #x_0 = torch.tensor([10, 20, 10], dtype=torch.float, device=device)\n",
        "        x_0 = torch.tensor([-2, -5, 25], dtype=torch.float, device=device)\n",
        "        eps = 0.01*0.934 # 0.934 is the Lyapunov Exponent\n",
        "        df = Lorenz\n",
        "\n",
        "    elif sys_name == \"Elicoidal\":\n",
        "        x_0 = torch.tensor([5, 5, 5], dtype=torch.float, device=device)\n",
        "        eps = 0.01\n",
        "        df = Elicoidal\n",
        "\n",
        "\n",
        "sys = Sys(x_0, df, eps, sys_type=sys_type)\n",
        "\n",
        "# No physical loss for discrete systems\n",
        "if sys_type == \"discrete\":\n",
        "  alpha = 0\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eJH4-Yc6kT_a"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pre-Training\n",
        "\n",
        "optimizer = get_model_optimizer(model, opt_type)\n",
        "\n",
        "## Pre-Training\n",
        "#  Ridge regression: X*R'*(R*R' + tikhonov*I)^-1\n",
        "if pre_training:\n",
        "    h_i = h_0\n",
        "    x_i = x_0\n",
        "\n",
        "    sys.restart(x_i)\n",
        "\n",
        "    #washout\n",
        "    for i in range(washout):\n",
        "        _ , h_i = model(x_i, h_i)\n",
        "        sys.step()\n",
        "        x_i = sys.x\n",
        "    #sys.clock=0\n",
        "\n",
        "    R = h_i.clone().detach().unsqueeze(1)\n",
        "    X = x_i.clone().detach().unsqueeze(1)\n",
        "\n",
        "    model.train()\n",
        "    if for_hor == \"v\":\n",
        "        for_hor_t = Nt//2\n",
        "    elif for_hor == \"c\" or for_hor == \"p\":\n",
        "        for_hor_t = 1\n",
        "    else:\n",
        "        for_hor_t = for_hor\n",
        "    for i in range(1, pre_training_horizon):\n",
        "\n",
        "        # model prediction\n",
        "        _ , h_i = model(x_i,h_i)\n",
        "         \n",
        "        # ground truth model step\n",
        "        sys.step()\n",
        "        x_i = sys.x\n",
        "\n",
        "        X = torch.cat([X, x_i.clone().detach().unsqueeze(1)],dim=1)\n",
        "        R = torch.cat([R, h_i.clone().detach().unsqueeze(1)],dim=1)\n",
        "\n",
        "    # Wout = X*R'*(R*R' + tikhonov*I)^-1\n",
        "\n",
        "    Wout = torch.einsum(\"dn, nh -> dh\", X, R.t())\n",
        "    R_inv = torch.einsum(\"hn, nk -> hk\", R, R.t())\n",
        "\n",
        "    Wout = torch.einsum(\"dh, hk -> dk\", Wout, torch.inverse(R_inv + tikhonov*torch.eye(H, device=device)))\n",
        "    #Wout.requires_grad = True\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.fco.weight = torch.nn.Parameter(Wout)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h0qn_FJ2MKlh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh4WFBQGjFW9",
        "outputId": "882c0a82-23b2-4caa-a703-e8f8be945231",
        "cellView": "form"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "########## ESN\n",
            "\n",
            "Reproducibility:                42\n",
            "Using device:                   cpu\n",
            "Hidden dimension:               200\n",
            "AVG connectivity:               2.985\n",
            "Sigma_in:                       0.15\n",
            "Lambda:                         0.4\n",
            "Alpha:                          100\n",
            "Training horizon:               400\n",
            "Washout:                        100\n",
            "System type:                    Lorenz   [continuous]\n",
            "System dimension:               3\n",
            "Trained forecasting horizon:    1\n",
            "Epochs:                         10000\n",
            "Reservoir activation function:  ConvModTanh2\n",
            "Optimizer:                      Adam\n",
            "Ealry Stop:                     None\n",
            "Tikhonov:                       0.0001   [ p=2 ]\n",
            "Basin radius:                   0.01\n",
            "Save Training:                  False\n",
            "Pre-Trainig:                    True\n",
            "Pre-Training horizon:           20\n",
            "ESN number of parameters:       1005\n",
            "df collocation weight:          0.5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train epoch:  93%|█████████▎| 9325/10000 [3:16:17<14:06,  1.25s/it]"
          ]
        }
      ],
      "source": [
        "#@title Training\n",
        "\n",
        "\n",
        "print(\"########## ESN\\n\")\n",
        "print(f\"Reproducibility:                {reproducible}\")\n",
        "print(f'Using device:                   {device}') \n",
        "print(f\"Hidden dimension:               {H}\")\n",
        "print(f\"AVG connectivity:               {model.connectivity}\")\n",
        "print(f\"Sigma_in:                       {sigma_in}\")\n",
        "print(f\"Lambda:                         {lambda_coeff}\")\n",
        "print(f\"Alpha:                          {alpha}\")\n",
        "print(f\"Training horizon:               {Nt}\")\n",
        "print(f\"Washout:                        {washout}\")\n",
        "print(f\"System type:                    {sys_name}   [{sys_type}]\")\n",
        "print(f\"System dimension:               {dym_sys}\")\n",
        "print(f\"Trained forecasting horizon:    {for_hor}\")\n",
        "print(f\"Epochs:                         {epochs}\")\n",
        "print(f\"Reservoir activation function:  {activation}\")\n",
        "print(f\"Optimizer:                      {opt_type}\")\n",
        "print(f\"Ealry Stop:                     {early_stop}\")\n",
        "print(f\"Tikhonov:                       {tikhonov}   [ p={p_tikhonov} ]\")\n",
        "print(f\"Basin radius:                   {basin_r}\")\n",
        "print(f\"Save Training:                  {save_training}\")\n",
        "print(f\"Pre-Trainig:                    {pre_training}\")\n",
        "print(f\"Pre-Training horizon:           {pre_training_horizon}\")\n",
        "print(f'ESN number of parameters:       {count_parameters(model)}')\n",
        "print(f'df collocation weight:          {df_wg}\\n')\n",
        "\n",
        "\n",
        "########### TRAINING Phase\n",
        "\n",
        "\n",
        "optimizer = get_model_optimizer(model, opt_type)\n",
        "\n",
        "## Training\n",
        "\n",
        "num_print = 10      # Number of Loss values printed\n",
        "buff_print = \"\"\n",
        "loss_plotter = []\n",
        "weigths_norm_plt = []\n",
        "stopped = False\n",
        "for epoch in trange(epochs, desc=\"train epoch\"):\n",
        "    model.train()\n",
        "\n",
        "    h_i = h_0\n",
        "    x_i = (torch.rand([3], dtype=torch.float, device=device)*2-1)*basin_r + x_0\n",
        "\n",
        "    sys.restart(x_i)\n",
        "\n",
        "    #washout\n",
        "    for i in range(washout):\n",
        "        _ , h_i = model(x_i, h_i)\n",
        "        sys.step()\n",
        "        x_i = sys.x\n",
        "   \n",
        "    x_hat_i = x_i\n",
        "\n",
        "    Ed = 0\n",
        "    Ep = 0\n",
        "    x_prev = x_hat_i\n",
        "    if for_hor == \"v\":\n",
        "        for_hor_t = random.randint(a=1, b=Nt)\n",
        "    elif for_hor == \"c\":\n",
        "        for_hor_t = curriculum_gen(epoch, epochs, Nt)\n",
        "    elif for_hor == \"p\":\n",
        "        for_hor_t = phased_gen(epoch, epochs, Nt) #,cycles=4)\n",
        "    else:\n",
        "        for_hor_t = for_hor\n",
        "    for i in range(1, Nt+1):\n",
        "        if for_hor_t != \"n\" and i % for_hor_t == 0:\n",
        "            x_hat_i = x_i\n",
        "\n",
        "        x_hat_i, h_i = model(x_hat_i,h_i)\n",
        "\n",
        "        sys.step()\n",
        "        x_i = sys.x\n",
        "\n",
        "        Ep += ((x_hat_i - x_prev) - df(sys.t0+sys.clock*sys.eps, (df_wg*x_hat_i + (1-df_wg)*x_prev))*sys.eps)**2\n",
        "        Ed += (x_hat_i - x_i)**2        \n",
        "        x_prev = x_hat_i\n",
        "\n",
        "\n",
        "    # p-regularization \n",
        "    Ed += tikhonov*torch.norm(model.fco.weight, p=p_tikhonov, dim=1)\n",
        "    Ed = torch.sum(Ed/Nt)/dym_sys\n",
        "\n",
        "    # Physical constraint\n",
        "    Ep = torch.sum(Ep/Nt)/dym_sys\n",
        "    Ed += alpha*Ep\n",
        "\n",
        "    # updating print buffer\n",
        "    if epochs < num_print or epoch % (epochs//num_print) == 0:\n",
        "        buff_print += f\"\\nLoss [epoch {epoch}]: {Ed}\"\n",
        "\n",
        "    # updating buffers for plots    \n",
        "    loss_plotter.append(Ed.detach())\n",
        "    weigths_norm_plt.append(tikhonov*torch.sum(torch.norm(model.fco.weight, p=p_tikhonov, dim=1)).detach())\n",
        "\n",
        "    # early stopping\n",
        "    if early_stop != None and Ed < early_stop:\n",
        "        stopped = True\n",
        "        break\n",
        "\n",
        "    # backpropagation and optimization\n",
        "    Ed.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "if epochs < num_print or epoch % (epochs//num_print) != 0:\n",
        "    buff_print += f\"\\nLoss [epoch {epoch}]: {Ed}\"\n",
        "print(buff_print)\n",
        "if stopped:\n",
        "    print(f\"\\nEarly Stop:  {Ed} < {early_stop}   [epoch: {epoch}]\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing\n",
        "\n",
        "\n",
        "########### TEST Phase\n",
        "\n",
        "model.eval()\n",
        "Nt_test = Nt*2  # Horizon in test phase\n",
        "threshold = 0.2 # Error threshold for predicatbility horizon\n",
        "sys.restart(x_0)\n",
        "x_i = x_0\n",
        "h_i = h_0\n",
        "\n",
        "#washout\n",
        "for i in range(washout):\n",
        "    _ , h_i = model(x_i, h_i)\n",
        "    sys.step()\n",
        "    x_i = sys.x\n",
        "\n",
        "\n",
        "xn_hat_i = x_i\n",
        "xt_hat_i = x_i\n",
        "\n",
        "h1_i = h_i\n",
        "hn_i = h_i\n",
        "ht_i = h_i\n",
        "\n",
        "\n",
        "x_sys = x_i.cpu().unsqueeze(0)\n",
        "x_for_1 = x_i.cpu().unsqueeze(0)\n",
        "x_for_n = x_i.cpu().unsqueeze(0)\n",
        "x_for_t = x_i.cpu().unsqueeze(0)\n",
        "\n",
        "\n",
        "time_avg=0\n",
        "error_plot_1for = torch.zeros([1])\n",
        "error_plot_nfor = torch.zeros([1])\n",
        "error_plot_tfor = torch.zeros([1])\n",
        "\n",
        "\n",
        "if for_hor == \"v\":\n",
        "    #for_hor_t = random.randint(a=1, b=Nt)\n",
        "    for_hor_t = Nt//2\n",
        "elif for_hor == \"c\" or for_hor == \"p\":\n",
        "    for_hor_t = 1\n",
        "else:\n",
        "    for_hor_t = for_hor\n",
        "\n",
        "for i in range(1, Nt_test+1):\n",
        "\n",
        "    # 1 step forecasting\n",
        "    x1_hat_i, h1_i = model(x_i, h1_i)\n",
        "\n",
        "    # n step forecasting\n",
        "    xn_hat_i, hn_i = model(xn_hat_i, hn_i)\n",
        "\n",
        "    # trained t step forecasting\n",
        "    if for_hor_t != \"n\" and i % for_hor_t == 0:\n",
        "        xt_hat_i = x_i\n",
        "    xt_hat_i, ht_i = model(xt_hat_i, ht_i)\n",
        "\n",
        "    # ground truth system\n",
        "    sys.step()\n",
        "    x_i = sys.x\n",
        "\n",
        "    time_avg += torch.norm(x_i)**2\n",
        "\n",
        "    error_plot_1for=torch.cat([error_plot_1for,torch.norm(x_i-x1_hat_i).detach().unsqueeze(0).cpu()],dim=0)\n",
        "    error_plot_nfor=torch.cat([error_plot_nfor,torch.norm(x_i-xn_hat_i).detach().unsqueeze(0).cpu()],dim=0)\n",
        "    error_plot_tfor=torch.cat([error_plot_tfor,torch.norm(x_i-xt_hat_i).detach().unsqueeze(0).cpu()],dim=0)\n",
        "\n",
        "\n",
        "    x_sys = torch.cat([x_sys, x_i.unsqueeze(0).cpu()],dim=0)\n",
        "    x_for_1 = torch.cat([x_for_1, x1_hat_i.unsqueeze(0).detach().cpu()],dim=0)\n",
        "    x_for_n = torch.cat([x_for_n, xn_hat_i.unsqueeze(0).detach().cpu()],dim=0)\n",
        "    x_for_t = torch.cat([x_for_t, xt_hat_i.unsqueeze(0).detach().cpu()],dim=0)\n",
        "\n",
        "time_avg = (time_avg/Nt_test)**0.5\n",
        "error_plot_1for = error_plot_1for/time_avg\n",
        "error_plot_nfor = error_plot_nfor/time_avg\n",
        "error_plot_tfor = error_plot_tfor/time_avg\n",
        "\n",
        "print(\"\\n\\n\\n########## Testing Phase:\\n\")\n",
        "print(f\"Tested horizon:                 {Nt_test}\")\n",
        "print(f\"Predictability threshold:       {threshold}\")\n",
        "leng = len(error_plot_1for)-1\n",
        "for k,v in enumerate(error_plot_1for):\n",
        "    if v>threshold or k==leng:\n",
        "        print(f\"Predictability Horizon (1-for): {k} ({k*sys.eps} sec) | washout: {washout}\")\n",
        "        break\n",
        "leng = len(error_plot_nfor)-1\n",
        "for k,v in enumerate(error_plot_nfor):\n",
        "    if v>threshold or k==leng:\n",
        "        print(f\"Predictability Horizon (n-for): {k} ({k*sys.eps} sec) | washout: {washout}\")\n",
        "        break\n",
        "leng = len(error_plot_tfor)-1\n",
        "for k,v in enumerate(error_plot_tfor):\n",
        "    if v>threshold or k==leng:\n",
        "        print(f\"Predictability Horizon (t-for): {k} ({k*sys.eps} sec) | washout: {washout}\")\n",
        "        break\n",
        "\n",
        "\n",
        "########### SAVE\n",
        "\n",
        "plots = {}\n",
        "data = {}\n",
        "plots[\"loss\"] = loss_plotter\n",
        "plots[\"weigths_norm\"] = weigths_norm_plt\n",
        "plots[\"error_1-for\"] = error_plot_1for\n",
        "plots[\"error_n-for\"] = error_plot_nfor\n",
        "plots[\"error_t-for\"] = error_plot_tfor\n",
        "plots[\"ground_truth_sys\"] = x_sys\n",
        "plots[\"1-forecasting\"] = x_for_1\n",
        "plots[\"n-forecasting\"] = x_for_n\n",
        "plots[\"trained-forecasting\"] = x_for_t\n",
        "\n",
        "if save_training:\n",
        "\n",
        "    # Saving plots and configs\n",
        "\n",
        "    data[\"reproducible\"] = reproducible\n",
        "    data[\"device\"] = str(device)\n",
        "    data[\"H\"] = H\n",
        "    data[\"connectivity\"] = model.connectivity\n",
        "    data[\"sigma_in\"] = sigma_in\n",
        "    data[\"lambda_coeff\"] = lambda_coeff\n",
        "    data[\"alpha\"] = alpha\n",
        "    data[\"Nt\"] = Nt\n",
        "    data[\"sys_name\"] = sys_name\n",
        "    data[\"sys_type\"] = sys_type\n",
        "    data[\"dym_sys\"] = dym_sys\n",
        "    data[\"for_hor\"] = for_hor\n",
        "    data[\"epochs\"] = epochs\n",
        "    data[\"activation\"] = activation\n",
        "    data[\"opt_type\"] = opt_type\n",
        "    data[\"early_stop\"] = early_stop\n",
        "    data[\"tikhonov\"] = tikhonov\n",
        "    data[\"p_tikhonov\"] = p_tikhonov\n",
        "    data[\"basin_r\"] = basin_r\n",
        "    data[\"washout\"] = washout\n",
        "    data[\"pre_training\"] = pre_training\n",
        "    data[\"pre_training_horizon\"] = pre_training_horizon\n",
        "    data[\"parameters count\"] = count_parameters(model)\n",
        "    data[\"Nt_test\"] = Nt_test\n",
        "    data[\"threshold\"] = threshold \n",
        "\n",
        "    save_hidden(h_0.detach().cpu())\n",
        "    save_initial(x_0.detach().cpu())\n",
        "    save_W(model.W.detach().cpu())\n",
        "    save_Win(model.Win.detach().cpu())\n",
        "    save(model, data, plots)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xepx-4XTL30E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "fU8O5ydrjrKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loss\n",
        "#import plotly.graph_objs as go\n",
        "#import plotly\n",
        "\n",
        "plt.plot(loss_plotter, label=\"loss\")\n",
        "plt.plot(weigths_norm_plt, label=\"weigths norm\")\n",
        "plt.title(f\"Loss value\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(f\"L(t)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bQR88Vn4jVuv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Error\n",
        "\n",
        "plt.plot(error_plot_1for, label=\"Err-1-for\", color=\"red\")\n",
        "plt.plot(error_plot_nfor, label=\"Err-n-for\", color=\"green\")\n",
        "plt.plot(error_plot_tfor, label=\"Err-t-for\", color=\"blue\")\n",
        "plt.title(f\"Forcasting Error and Predictability Horizon\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(f\"E(t)\")\n",
        "for k,v in enumerate(error_plot_1for):\n",
        "    if v>threshold:\n",
        "        plt.scatter(k, error_plot_1for[k], color=\"red\") # Predictability Horizon for 1-for\n",
        "        break\n",
        "for k,v in enumerate(error_plot_nfor):\n",
        "    if v>threshold:\n",
        "        plt.scatter(k, error_plot_nfor[k], color=\"green\") # Predictability Horizon for n-for\n",
        "        break\n",
        "for k,v in enumerate(error_plot_tfor):\n",
        "    if v>threshold:\n",
        "        plt.scatter(k, error_plot_tfor[k], color=\"blue\") # Predictability Horizon for t-for\n",
        "        break\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8VbtDKoHmMS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot x0 vs t\n",
        "\n",
        "k=0\n",
        "plt.title(f\"t-Forecasting\")\n",
        "plt.plot(x_sys[:,k], color=\"blue\", label=\"ground truth\")\n",
        "plt.plot(x_for_1[:,k], color=\"red\", label=\"1-forecasting\")\n",
        "plt.plot(x_for_n[:,k], color=\"green\",label=\"n-forecasting\")\n",
        "#if for_hor != \"n\" and for_hor != 1:\n",
        "#    plt.plot(x_for_t, color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.plot(x_for_t[:,k], color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(f\"x{k}(t)\")\n",
        "plt.legend()\n",
        "plt.scatter(Nt, x_sys[Nt, k], color=\"black\") # training horizon position\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Lbnl40zcjaXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot x1 vs t\n",
        "\n",
        "\n",
        "k=1\n",
        "plt.title(f\"t-Forecasting\")\n",
        "plt.plot(x_sys[:,k], color=\"blue\", label=\"ground truth\")\n",
        "plt.plot(x_for_1[:,k], color=\"red\", label=\"1-forecasting\")\n",
        "plt.plot(x_for_n[:,k], color=\"green\",label=\"n-forecasting\")\n",
        "#if for_hor != \"n\" and for_hor != 1:\n",
        "#    plt.plot(x_for_t, color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.plot(x_for_t[:,k], color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(f\"x{k}(t)\")\n",
        "plt.legend()\n",
        "plt.scatter(Nt, x_sys[Nt, k], color=\"black\") # training horizon position\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ikDql1HRjb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot x2 vs t\n",
        "\n",
        "\n",
        "k=2\n",
        "plt.title(f\"t-Forecasting\")\n",
        "plt.plot(x_sys[:,k], color=\"blue\", label=\"ground truth\")\n",
        "plt.plot(x_for_1[:,k], color=\"red\", label=\"1-forecasting\")\n",
        "plt.plot(x_for_n[:,k], color=\"green\",label=\"n-forecasting\")\n",
        "#if for_hor != \"n\" and for_hor != 1:\n",
        "#    plt.plot(x_for_t, color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.plot(x_for_t[:,k], color=\"violet\",label=f\"{for_hor}-forecasting (trained)\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(f\"x{k}(t)\")\n",
        "plt.legend()\n",
        "plt.scatter(Nt, x_sys[Nt, k], color=\"black\") # training horizon position\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E0odjBvdjdcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot 3D - Ground Truth vs T-forecasting\n",
        "\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.plot3D(x_for_t[:,0].numpy(), x_for_t[:,1].numpy(), x_for_t[:,2].numpy(), 'red', label=\"gnd\")\n",
        "ax.plot3D(x_sys[:,0].numpy(), x_sys[:,1].numpy(), x_sys[:,2].numpy(), 'blue', label=\"t-for\")\n",
        "plt.title(\"3D plot\")\n",
        "ax.scatter(x_sys[0,0],x_sys[0,1],x_sys[0,2], color=\"green\") # initial position\n",
        "ax.scatter(x_sys[Nt,0],x_sys[Nt,1],x_sys[Nt,2], color=\"black\") # training horizon position\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# trace2 = go.Scatter3d(\n",
        "#     x=x_sys[:,0],\n",
        "#     y=x_sys[:,1],\n",
        "#     z=x_sys[:,2],\n",
        "#     mode='lines',\n",
        "#     name='lines'\n",
        "# )\n",
        "# fig = go.Figure(data=[, weigths_norm_plt])\n",
        "# plotly.offline.iplot(fig, filename='simple-3d-scatter')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qR-r4Nt5jfKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot 3D  Ground Truth vs T-forcasting\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#x_sys = x_sys.numpy()\n",
        "\n",
        "\n",
        "\n",
        "d = {'x': x_sys[:,0], 'y': x_sys[:,1], 'z': x_sys[:,2], 'target':'gnd'}\n",
        "dgnd = pd.DataFrame(data=d)\n",
        "\n",
        "d = {'x': x_for_t[:,0], 'y': x_for_t[:,1], 'z': x_for_t[:,2], 'target':'t-for'}\n",
        "dtfor = pd.DataFrame(data=d)\n",
        "\n",
        "df = pd.concat([dgnd, dtfor])\n",
        "\n",
        "\n",
        "\n",
        "fig = px.line_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"target\")#, line_dash=\"target\")\n",
        "\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OcmAm2JS1IJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot 3D  Ground Truth vs 1-forcasting\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#x_sys = x_sys.numpy()\n",
        "\n",
        "\n",
        "\n",
        "d = {'x': x_sys[:,0], 'y': x_sys[:,1], 'z': x_sys[:,2], 'target':'gnd'}\n",
        "dgnd = pd.DataFrame(data=d)\n",
        "\n",
        "d = {'x': x_for_t[:,0], 'y': x_for_t[:,1], 'z': x_for_1[:,2], 'target':'t-for'}\n",
        "dtfor = pd.DataFrame(data=d)\n",
        "\n",
        "df = pd.concat([dgnd, dtfor])\n",
        "\n",
        "\n",
        "\n",
        "fig = px.line_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"target\")#, line_dash=\"target\")\n",
        "\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z3havouaFoLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot 3D  Ground Truth vs N-forcasting\n",
        "\n",
        "d = {'x': x_sys[:,0], 'y': x_sys[:,1], 'z': x_sys[:,2], 'target':'gnd'}\n",
        "dgnd = pd.DataFrame(data=d)\n",
        "\n",
        "d = {'x': x_for_n[:,0], 'y': x_for_n[:,1], 'z': x_for_n[:,2], 'target':'n-for'}\n",
        "dnfor = pd.DataFrame(data=d)\n",
        "\n",
        "df = pd.concat([dgnd, dnfor])\n",
        "\n",
        "fig = px.line_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"target\")\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QuYJ5Vg78ZP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}